~~Full Code used for D213 Task 2 Submission~~

#Import packages that will be used for this analysis
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns

from sklearn.datasets import make_classification
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.svm import SVC

import re
from collections import Counter
import spacy
from gensim.corpora.dictionary import Dictionary
from gensim.models.tfidfmodel import TfidfModel
import nltk
from nltk.tokenize import word_tokenize
from collections import Counter
from gensim.models.tfidfmodel import TfidfModel
from nltk.corpus import stopwords
import random
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D, LSTM, Dropout, SimpleRNN
from keras.callbacks import History 
from sklearn.model_selection import train_test_split

#Import and concatenate the datasets
amazon = pd.read_csv('amazon_cells_labelled.txt', delimiter='\t', header=None, names=['review', 'rating'])
imbd = pd.read_csv('imdb_labelled.txt', delimiter='\t', header=None, names=['review', 'rating'])
yelp =  pd.read_csv('yelp_labelled.txt', delimiter='\t', header=None, names=['review', 'rating'])
df = pd.concat([amazon, imbd, yelp])
reviews_df = df
reviews_df.reset_index(inplace=True)
reviews_df.sample(10)

#Visually inspect the dataframe
reviews_df.shape
reviews_df.info()
reviews_df.describe()
reviews_df['rating'].value_counts()
reviews_df.isna().sum()
reviews_df.review.sample(10)

#Perform exploratory data analysis on the concatenated dataset
reviews = reviews_df['review']
char_list = []
for review in reviews:
    for word in word_tokenize(review.lower()):
        for char in word:
            if char not in char_list:
                char_list.append(char)

alpha = '[a-zA-Z]'
num = '[0-9]'
alpha_chars = []
num_chars = []
nonal_num_chars = []

for char in char_list:
    try:
        try:
            alpha_chars.append(re.match(alpha, char)[0])
        except:
            num_chars.append(re.match(num, char)[0])    
    except:
        nonal_num_chars.append(char)
        
print('All alpha Characters:')
print(alpha_chars)
print('There are ',len(alpha_chars),' unique english letters in this dataset')
print(' ')

print('All numeric Characters:')
print(num_chars)
print('There are ',len(num_chars),' unique numerical characters in this dataset')
print(' ')
      
print('All non-alphanumeric characters:')
print(nonal_num_chars)
print('There are ',len(nonal_num_chars),' unique special characters in this dataset')

#Divide the reviews into seperate words
#Convert words into lowercase
#Elimnate stopwords
#lemmatize the list
rev_list = []
rev_len = []
stop_words = stopwords.words('english')

for review in df.review:
    review = re.sub("[^a-zA-Z\s]", "", review)
    review = review.lower()
    review = nltk.word_tokenize(review)
    review = [word for word in review if not word in stop_words]
    lemma = nltk.WordNetLemmatizer()
    review = [lemma.lemmatize(word) for word in review]
    length = len(review)
    rev_len.append(length)
    rev_list.append(review)

n = random.randint(0, len(rev_list))
rev_list = np.asarray(rev_list, dtype=object)
print(rev_list[n])

#Convert words into numerical values
#Sequence the tokenizer
tokenizer = Tokenizer(lower=True)
tokenizer.fit_on_texts(rev_list)
word_index = tokenizer.word_index
word_counts = list(tokenizer.word_counts.items())
word_counts.sort(key=lambda y: y[1], reverse=True)
vocab_size = len(tokenizer.word_index)+1

max_seq_emb = int(round(vocab_size ** (1/4))) #, 0))
max_len = len(max(rev_list, key=len))

sequence = tokenizer.texts_to_sequences(rev_list)

#Padding
padded_sequence = pad_sequences(sequence, maxlen=max_len, padding='post', truncating='post')
print('Vocabulary size: ',vocab_size)
print('max sequence embed: ', max_seq_emb)
print('max review length: ', max_len)

#Provide a single padded sequence
n = random.randint(0, len(rev_list))
print('Original Review:')
print('"', df.review[n], '"')
print('____________________________________________________________')
print('')

print('Review split, lemmatized and stop words removed')
print(rev_list[n])
print('____________________________________________________________')
print('')


print('Review tokenized, sequenced and padded:')
print(padded_sequence[n])

#Create the model, using binary cross entropy for the loss function and sigmoid for final layer activation
keras.backend.clear_session()

model = keras.Sequential()
model.add(Embedding(vocab_size, max_seq_emb))
model.add(GlobalAveragePooling1D())
model.add(Dense(50, activation="sigmoid"))
model.add(Dense(1, activation="sigmoid"))

model.compile(loss='BinaryCrossentropy',
              optimizer='adam',
              metrics=['accuracy'])

model.summary()

#Create the training and test splits
X_train, X_test, y_train, y_test = train_test_split(padded_sequence,
                                                    np.array(df.rating),
                                                    test_size=0.2,
                                                    random_state=42)

pd.DataFrame(X_train).to_csv('X_training_data.csv')
pd.DataFrame(X_test).to_csv('X_testing_data.csv')
pd.DataFrame(y_train).to_csv('y_training_data.csv')
pd.DataFrame(y_test).to_csv('y_testing_data.csv')

#Showcase the size and shape of the training and test splits
print('Size and shape of the training data set:')
print('Training data X values (reviews text) size = ',X_train.size, ' and shape = ', X_train.shape)
print('Training data Y values (review ratings) size = ',y_train.size, ' and shape = ', y_train.shape)

print('')
print('')

print('Size and shape of the training data set:')
print('Training data X values (reviews text) size = ',X_test.size, ' and shape = ', X_test.shape)
print('Training data Y values (review ratings) size = ',y_test.size, ' and shape = ', y_test.shape)

#Provide visualizations of the modelâ€™s training process
stop_monitor = keras.callbacks.EarlyStopping(patience=5)

history = model.fit(X_train, y_train,
                    epochs=1000,
                    validation_split=.2,
                    shuffle=True,
                    verbose=2,
                    callbacks=stop_monitor)

#Print the score of the Final Model Loss and Final Model Accuracy
score = model.evaluate(X_test, y_test, verbose=1)

print('Final Model Loss: ', round(score[0],5))
print('Final Model Accuracy: ', round(score[1]*100, 2),'%')

#Plot the Training and Validation data loss and accuracy
plt.figure(figsize=(15,15))

plt.subplot(2,1,1)
plt.plot(history.history['loss'], label='Training Loss', c='r')
plt.plot(history.history['accuracy'], label='Training Accuracy', c='blue')
plt.xlabel('epochs')
plt.legend()
plt.title('Training data, loss and accuracy')

plt.subplot(2,1,2)
plt.plot(history.history['val_loss'], label='Validation Loss', c='r')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy', c='blue')
plt.xlabel('epochs')
plt.legend()
plt.title('Validation data, loss and accuracy')

plt.show;

#Plot the Loss and Accuracy of Valication and Training data
plt.figure(figsize=(15,15))

plt.subplot(2,1,1)
plt.plot(history.history['loss'], label='Training Loss', c='green')
plt.plot(history.history['val_loss'], label='Validation Loss', c='blue')

plt.xlabel('epochs')
plt.legend()
plt.title('Loss of Validation and Training data')

plt.subplot(2,1,2)
plt.plot(history.history['accuracy'], label='Training Accuracy', c='green')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy', c='blue')
plt.xlabel('epochs')
plt.legend()
plt.title('Accuracy  of Validation and Training data')

plt.show();

#Generate a confusion matrix to see how correctly the model can predict positive and negative reviews
predictions = model.predict(X_test)
predictions = np.round(predictions,0).astype(int)

con_mat = confusion_matrix(y_test, predictions)
sns.set(rc={'figure.figsize':(12,12)})
sns.heatmap(
    con_mat, annot=True,
    fmt='d', cbar=False,
    cmap='mako').set(
    ylabel='Actual',
    xlabel='Predictions');

#Save the trained network within the neural network
model.save('D213_Task2_Sentiment_Analys.keras')