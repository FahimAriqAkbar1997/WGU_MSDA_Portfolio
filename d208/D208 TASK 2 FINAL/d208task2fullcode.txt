~~Full Code used for D208 Task 2 Submission~~

# Import packages that will be used for the logistics regression analysis
import pylab
import seaborn as sb
sb.set(style="white")
sb.set(style="whitegrid", color_codes=True)
import sklearn
from sklearn.metrics import confusion_matrix
from sklearn import preprocessing
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
from sklearn.metrics import classification_report
from sklearn import metrics
import matplotlib.pyplot as plt
plt.rc("font", size=14)
import numpy as np
import scipy.stats as stats
import statsmodels.api as sm
import statsmodels.formula.api as smf
from statsmodels.graphics.mosaicplot import mosaic
from statsmodels.stats.outliers_influence import variance_inflation_factor
from IPython.core.display import HTML
from IPython.display import display
import pandas as pd
from pandas.api.types import CategoricalDtype
from pandas import Series, DataFrame
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE

# Import data set that will be used for the logistics regression analysis
pd.set_option('display.max_columns', None)
df = pd.read_csv (r'C:\Users\fahim\Documents\0_WGUDocuments\d208\1medical_clean.csv')
# Check data types and number of values, as well as overall size of dataframe
df.info()

# Visually inspect dataframe to facilitate exploration, spot problems
pd.set_option("display.max_columns", None)
df.head(5)

#check if there is any duplicate data entries present in columns
df[df.duplicated()]

# check if there are any duplicated columns in the data set - if there are none then the output should be False
df.columns.duplicated().any()

# check if there are any duplicated rows in the data set - if there are none then the output should be False
df.duplicated().any()

#Summary Statistics
df.Age.describe()
df.Gender.value_counts()
df.VitD_levels.describe()
df.Initial_admin.value_counts().sort_index()
df.HighBlood.value_counts()
df.Complication_risk.value_counts().sort_index()
df.Overweight.value_counts()
df.BackPain.value_counts()
df.Diabetes.value_counts()
df.Asthma.value_counts()
df.Initial_days.describe()
df.Initial_days.nlargest(n=20)
df.Arthritis.value_counts()

# Data Preparation for analysis
# Convert column to category from string
df["TimeZone"] = df["TimeZone"].astype("category")
# Reformat column representing currency in USD to 3 decimal places from 6
df["Income"] = df["Income"].astype(int)
# Convert column to category from string
df["Marital"] = df["Marital"].astype("category")
# Convert column to category from string
df["Gender"] = df["Gender"].astype("category")

# Convert categorical yes/no values to numeric 1/0 values
df = df.replace(to_replace = ['Yes','No'],value = [1,0])

# Perform one-hot encoding
# Generate columns of dummy values for dataframe's Gender column
gender_temp_df = pd.get_dummies(data=df["Gender"], drop_first=True)
# Generate columns of dummy values for dataframe's Initial_admin column
initial_admit_temp_df = pd.get_dummies(data=df["Initial_admin"], drop_first=True)
# Generate columns of dummy values for dataframe's Complication_risk column
comp_risk_temp_df = pd.get_dummies(data=df["Complication_risk"], drop_first=True)
# Create the new df with the variables used for this analysis
regress_df = df[["Age", "VitD_levels", "HighBlood", "Overweight", "Arthritis", "Diabetes", "BackPain", "Asthma", "Initial_days"]]
# Generate and apply new Pythonic names for ease of use
pythonic_columns = ["age", "vit_d_level", "high_bp", "overweight", "arthritis", "diabetes", "back_pain", "asthma", "days_hospitalized"]
regress_df.set_axis(pythonic_columns, axis=1, inplace=True)
# Insert the generated dummy variables to new dataframe, placing them in the same order as the original dataframe
# Dummies for Complication Risk
regress_df.insert(4, "comp_risk_medium", comp_risk_temp_df.Medium)
regress_df.insert(4, "comp_risk_low", comp_risk_temp_df.Low)
# Dummies for Initial Admit
regress_df.insert(3, "initial_admit_emerg", initial_admit_temp_df["Emergency Admission"])
regress_df.insert(3, "initial_admit_observ", initial_admit_temp_df["Observation Admission"])
# Dummies for Gender
regress_df.insert(2, "gender_nonbinary", gender_temp_df.Nonbinary)
regress_df.insert(2, "gender_male", gender_temp_df.Male)
# Check resulting dataframe
regress_df

#Bivariate distribution of Arthritis
plt.figure(figsize = [16,5])
plt.title('Distribution of Patients with Arthritis')
arthritis_counts = regress_df.arthritis.value_counts()
arthritis_labels = ["No arthritis", "Arthritis"]
plt.pie(arthritis_counts, labels=arthritis_labels, autopct='%1.1f%%', startangle=90, counterclock=False)
plt.axis('square');

#Univariate and bivariate distribution of Age
plt.figure(figsize = [16,5])
plt.suptitle("Visual exploration of Patient's Age")

# LEFT plot: Univariate exploration of age
plt.subplot(1, 2, 1)
plt.title('Distribution of Patient Age')
bins = np.arange(0, 100, 1)
plt.hist(data=regress_df, x="age", bins=bins)
plt.xlabel('Patient Age')
plt.ylabel("Number of Patients");

# RIGHT plot: Bivariate exploration of age vs arthritis
plt.subplot(1, 2, 2)
plt.title("Relationship of Age vs Arthritis")
sb.violinplot(data = regress_df, x="age", y="arthritis", orient='h')
plt.xlabel("Patient Age")
plt.ylabel("Patient Arthritis")
plt.yticks([0,1], ["False", "True"]);

plt.figure(figsize = [16,5])
plt.suptitle("Visual exploration of Patient's Age")

# LEFT plot: Univariate exploration of age
plt.subplot(1, 2, 1)
plt.title('Distribution of Patient Age')
bins = np.arange(0, 100, 1)
plt.hist(data=regress_df, x="age", bins=bins)
plt.xlabel('Patient Age')
plt.ylabel("Number of Patients");

# RIGHT plot: Bivariate exploration of age vs arthritis
plt.subplot(1, 2, 2)
plt.title("Relationship of Age vs Arthritis")
sb.histplot(data = regress_df, x="age", hue="arthritis", bins=bins, kde=True, multiple="stack")
plt.legend(title="Arthritis", labels=["Yes", "No"])
plt.xlabel("Patient Age")
plt.ylabel("Number of Patients");

#Univariate and bivariate distribution of Gender
plt.figure(figsize = [16,5])
plt.suptitle("Visual exploration of Patient's Gender")

# LEFT plot: Univariate exploration of num_children
plt.subplot(1, 2, 1)
plt.title("Patient Gender Distribution")
gender_counts = df["Gender"].value_counts()
plt.pie(gender_counts, labels=gender_counts.index, autopct='%1.1f%%', startangle=90, counterclock = False)
plt.axis('square');

# RIGHT plot: Bivariate exploration of num_children vs arthritis
plt.subplot(1, 2, 2)
plt.title("Relationship of Gender vs Hospitalization Length")
sb.countplot(data = df, x="Gender", hue="BackPain")
plt.legend(["No Arthritis", "Arthritis"])
plt.xlabel("Patient Gender")
plt.ylabel("Number of Patients");

#Univariate and bivariate distribution of Vitamin D Level
plt.figure(figsize = [16,5])
plt.suptitle("Visual exploration of Patient's Vitamin D Level")

# LEFT plot: Univariate exploration of vit_d_level
plt.subplot(1, 2, 1)
plt.title('Distribution of Patient Vitamin D Level')
bins = np.arange(9, 27, 0.5)
plt.hist(data=regress_df, x="vit_d_level", bins=bins)
plt.xlabel('Patient Vitamin D Level')
plt.ylabel("Number of Patients");

# RIGHT plot: Bivariate exploration of vit_d_level vs arthritis
plt.subplot(1, 2, 2)
plt.title("Relationship of Vitamin D Level vs Arthritis")
bins_y = np.arange(0, 1.25, 0.5)
plt.hist2d(data= regress_df, x="vit_d_level", y="arthritis", bins=[bins, bins_y], cmap= "viridis_r")
plt.colorbar()
plt.xlabel("Patient Vitamin D Level")
plt.ylabel("Patient Arthritis")
plt.yticks([0,1], ["False", "True"]);

#Univariate and bivariate distribution of Initial Admissions
plt.figure(figsize = [16,5])
plt.suptitle("Visual exploration of Patient's Initial Admissions")

# LEFT plot: Univariate exploration of initial_admin
plt.subplot(1, 2, 1)
plt.title("Patient Initial Admission Distribution")
init_admit_counts = df["Initial_admin"].value_counts()
plt.pie(init_admit_counts, labels=init_admit_counts.index, autopct='%1.1f%%', startangle=90, counterclock = False)
plt.axis('square');

# RIGHT plot: Bivariate exploration of Initial_admin vs arthritis
plt.subplot(1, 2, 2)
plt.title("Relationship of Initial Admission vs Arthritis")
sb.countplot(data = df, x="Initial_admin", hue="BackPain")
plt.legend(["No Arthritis", "Arthritis"])
plt.xlabel("Patient Admission Type")
plt.ylabel("Number of Patients");

#Univariate and bivariate distribution of High Blood pressure
# TOP plot: Univariate exploration of high bp
plt.title("Patient High Blood Pressure Distribution")
high_bp_counts = df["HighBlood"].value_counts()
plt.pie(high_bp_counts, labels=["Normal BP", "High BP"], autopct='%1.1f%%', startangle=90, counterclock = False)
plt.axis('square');

# BOTTOM plot: Bivariate exploration of high bp vs arthritis
temp_df = df[["HighBlood", "BackPain"]].copy()
high_bp_map = {1 : "HighBP", 0: "NormBP"}
arthritis_map = {1 : "Arthritis", 0: "No Arthritis"}
temp_df["HighBlood"] = temp_df["HighBlood"].map(high_bp_map)
temp_df["BackPain"] = temp_df["BackPain"].map(arthritis_map)
mosaic(temp_df, ["HighBlood", "BackPain"])
plt.suptitle("Relationship of High Blood Pressure vs Arthritis");

#Univariate and bivariate distribution of Complication Risk
plt.figure(figsize = [16,5])
plt.suptitle("Visualization exploration of Patient's Complication Risk'")

# LEFT plot: Univariate exploration of complication_risk
plt.subplot(1, 2, 1)
plt.title("Patient Complication Risk Distribution")
comp_risk_counts = df["Complication_risk"].value_counts()
comp_risk_labels = ["Medium", "High", "Low"]
plt.pie(comp_risk_counts, labels=comp_risk_counts.index, autopct='%1.1f%%', startangle=90, counterclock = False)
plt.axis('square');

# RIGHT plot: Bivariate exploration of complication_risk vs arthritis
plt.subplot(1, 2, 2)
plt.title("Relationship of Complication Risk vs Arthritis")
sb.countplot(data = df, x="Complication_risk", hue="BackPain")
plt.legend(["No Arthritis", "Arthritis"])
plt.xlabel("Patient Admission Type")
plt.ylabel("Number of Patients");

#Univariate and bivariate distribution of Overweight
# TOP plot: Univariate exploration of overweight
plt.title("Patient Overweight Distribution")
overweight_counts = df["Overweight"].value_counts().sort_index()
plt.pie(overweight_counts, labels=["Not Overweight", "Overweight"], autopct='%1.1f%%', startangle=90, counterclock = False)
plt.axis('square');

# BOTTOM plot: Bivariate exploration of overweight vs arthritis
temp_df = df[["Overweight", "BackPain"]].copy()
overweight_map = {1 : "Overweight", 0: "NotOverweight"}
arthritis_map = {1 : "Arthritis", 0: "No Arthritis"}
temp_df["Overweight"] = temp_df["Overweight"].map(overweight_map)
temp_df["BackPain"] = temp_df["BackPain"].map(arthritis_map)
mosaic(temp_df, ["Overweight", "BackPain"])
plt.suptitle("Relationship of Overweight vs Arthritis");

#Univariate and bivariate distribution of BackPain
# TOP plot: Univariate exploration of arthritis
plt.title("Patient BackPain Distribution")
back_pain_counts = df["BackPain"].value_counts().sort_index()
plt.pie(arthritis_counts, labels=["No Back Pain", "Back Pain"], autopct='%1.1f%%', startangle=90, counterclock = False)
plt.axis('square');

# BOTTOM plot: Bivariate exploration of arthritis vs back_pain
temp_df = df[["Arthritis", "BackPain"]].copy()
back_pain_map = {1 : "Back Pain", 0: "No Back Pain"}
arthritis_map = {1 : "Arthritis", 0: "No Arthritis"}
temp_df["BackPain"] = temp_df["BackPain"].map(back_pain_map)
temp_df["Arthritis"] = temp_df["Arthritis"].map(arthritis_map)
mosaic(temp_df, ["Arthritis", "BackPain"])
plt.suptitle("Relationship of Back Pain vs Arthritis");

#Univariate and bivariate distribution of Diabetes
# TOP plot: Univariate exploration of diabetes
plt.title("Patient Diabetes Distribution")
diabetes_counts = df["Diabetes"].value_counts().sort_index()
plt.pie(diabetes_counts, labels=["No Diabetes", "Diabetes"], autopct='%1.1f%%', startangle=90, counterclock = False)
plt.axis('square');

# BOTTOM plot: Bivariate exploration of diabetes vs arthritis
temp_df = df[["Diabetes", "BackPain"]].copy()
diabetes_map = {1 : "Diabetes", 0: "No Diabetes"}
arthritis_map = {1 : "Arthritis", 0: "No Arthritis"}
temp_df["Diabetes"] = temp_df["Diabetes"].map(diabetes_map)
temp_df["BackPain"] = temp_df["BackPain"].map(arthritis_map)
mosaic(temp_df, ["Diabetes", "BackPain"])
plt.suptitle("Relationship of Diabetes vs Arthritis");

#Univariate and bivariate distribution of Asthma
# TOP plot: Univariate exploration of asthma
plt.title("Patient Asthma Distribution")
asthma_counts = df["Asthma"].value_counts()
plt.pie(asthma_counts, labels=["No Asthma", "Asthma"], autopct='%1.1f%%', startangle=90, counterclock = False)
plt.axis('square');

# BOTTOM plot: Bivariate exploration of asthma vs arthritis
temp_df = df[["Asthma", "BackPain"]].copy()
asthma_map = {1 : "Asthma", 0: "No Asthma"}
arthritis_map = {1 : "Arthritis", 0: "No Arthritis"}
temp_df["Asthma"] = temp_df["Asthma"].map(asthma_map)
temp_df["BackPain"] = temp_df["BackPain"].map(arthritis_map)
mosaic(temp_df, ["Asthma", "BackPain"])
plt.suptitle("Relationship of Asthma vs Arthritis");

#Univariate and bivariate distribution of Days Hospitalized
plt.figure(figsize = [16,5])
plt.suptitle("Visual exploration of Patient's Days Hospitalized")

# LEFT plot: Univariate exploration of days_hospitalized
plt.subplot(1, 2, 1)
plt.title('Distribution of Patient Days Hospitalized')
bins = np.arange(0, 75, 1)
plt.hist(data=regress_df, x="days_hospitalized", bins=bins)
plt.xlabel('Patient Days Hospitalized')
plt.ylabel("Number of Patients");

# RIGHT plot: Bivariate exploration of days_hospitalized vs arthritis
plt.subplot(1, 2, 2)
plt.title("Relationship of Days Hospitalized vs Arthritis")
bins_y = np.arange(0, 1.25, 0.5)
plt.hist2d(data= regress_df, x="days_hospitalized", y="arthritis", bins=[bins, bins_y], cmap= "viridis_r")
plt.colorbar()
plt.xlabel("Patient Days Hospitalized")
plt.ylabel("Patient Arthritis")
plt.yticks([0,1], ["False", "True"]);

# Save dataframe to CSV
df.to_csv('d208task2_full_clean.csv', index=False)

# Save dataframe to CSV
regress_df.to_csv('d208task2_red_clean.csv', index=False)

# Check for VIF to determine if variables should be eliminated due to high multicolinearity
# Selecting the features for VIF calculation
X = regress_df[["age", "gender_male", "gender_nonbinary", "vit_d_level", "initial_admit_observ", "initial_admit_emerg", "high_bp", "comp_risk_low", "comp_risk_medium", "overweight", "back_pain", "diabetes", "asthma", "days_hospitalized"]]

# Calculating VIF for each feature
vif_df = pd.DataFrame()
vif_df["feature"] = X.columns
vif_df["vif"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

print(vif_df)

#Create the Initial Logistic Regression model
y = regress_df.arthritis
X = regress_df[["age", "gender_male", "gender_nonbinary", "vit_d_level", "initial_admit_observ", "initial_admit_emerg", "high_bp", "comp_risk_low", "comp_risk_medium", "overweight", "back_pain", "diabetes", "asthma", "days_hospitalized"]].assign(const=1)
logit_model=sm.Logit(y,X)
result=logit_model.fit()
print(result.summary())

# Check for VIF to see if variables should be eliminated due to high multicolinearity
X = regress_df[["age", "gender_male", "gender_nonbinary", "vit_d_level", "initial_admit_observ", "initial_admit_emerg", "high_bp", "comp_risk_low", "comp_risk_medium", "overweight", "back_pain", "diabetes", "asthma", "days_hospitalized"]]

vif_df = pd.DataFrame()
vif_df["feature"] = X.columns

vif_df["VIF"] = [variance_inflation_factor(X.values, i)
for i in range(len(X.columns))]

print(vif_df)

# Eliminated vit_d_level (VIF = 16.271274), rerunning analysis to see if any VIF still above 10 
X = regress_df[["age", "gender_male", "gender_nonbinary", "initial_admit_observ", "initial_admit_emerg", "high_bp", "comp_risk_low", "comp_risk_medium", "overweight", "back_pain", "diabetes", "asthma", "days_hospitalized"]]

vif_df = pd.DataFrame()
vif_df["feature"] = X.columns

vif_df["VIF"] = [variance_inflation_factor(X.values, i)
for i in range(len(X.columns))]

print(vif_df)

# BACKWARD ELIMINATION # 1: Seek highest p-value above 0.10
y = regress_df.arthritis
X = regress_df[["age", "gender_male", "gender_nonbinary", "initial_admit_observ", "initial_admit_emerg", "high_bp", "comp_risk_low", "comp_risk_medium", "overweight", "back_pain", "diabetes", "asthma", "days_hospitalized"]].assign(const=1)
logit_model=sm.Logit(y,X)
result=logit_model.fit()
print(result.summary())

# BACKWARD ELIMINATION # 2: Seek highest p-value above 0.10 (eliminated initial_admit_emerg, p-value of 0.992)
y = regress_df.arthritis
X = regress_df[["age", "gender_male", "gender_nonbinary", "initial_admit_observ", "high_bp", "comp_risk_low", "comp_risk_medium", "overweight", "back_pain", "diabetes", "asthma", "days_hospitalized"]].assign(const=1)
logit_model=sm.Logit(y,X)
result=logit_model.fit()
print(result.summary())

# BACKWARD ELIMINATION # 3: Seek highest p-value above 0.10 (eliminated initial_admit_observ, p-value of 0.994)
y = regress_df.arthritis
X = regress_df[["age", "gender_male", "gender_nonbinary", "high_bp", "comp_risk_low", "comp_risk_medium", "overweight", "back_pain", "diabetes", "asthma", "days_hospitalized"]].assign(const=1)
logit_model=sm.Logit(y,X)
result=logit_model.fit()
print(result.summary())

# BACKWARD ELIMINATION # 4: Seek highest p-value above 0.10 (eliminated overweight, p-value of 0.686)
y = regress_df.arthritis
X = regress_df[["age", "gender_male", "gender_nonbinary", "high_bp", "comp_risk_low", "comp_risk_medium", "back_pain", "diabetes", "asthma", "days_hospitalized"]].assign(const=1)
logit_model=sm.Logit(y,X)
result=logit_model.fit()
print(result.summary())

# BACKWARD ELIMINATION # 5: Seek highest p-value above 0.10 (eliminated asthma, p-value of 0.518)
y = regress_df.arthritis
X = regress_df[["age", "gender_male", "gender_nonbinary", "high_bp", "comp_risk_low", "comp_risk_medium", "back_pain", "diabetes", "days_hospitalized"]].assign(const=1)
logit_model=sm.Logit(y,X)
result=logit_model.fit()
print(result.summary())

# BACKWARD ELIMINATION # 6: Seek highest p-value above 0.10 (eliminated age, p-value of 0.461)
y = regress_df.arthritis
X = regress_df[["gender_male", "gender_nonbinary", "high_bp", "comp_risk_low", "comp_risk_medium", "back_pain", "diabetes", "days_hospitalized"]].assign(const=1)
logit_model=sm.Logit(y,X)
result=logit_model.fit()
print(result.summary())

# BACKWARD ELIMINATION # 7: Seek highest p-value above 0.10 (eliminated gender_male, p-value of 0.357)
y = regress_df.arthritis
X = regress_df[["gender_nonbinary", "high_bp", "comp_risk_low", "comp_risk_medium", "back_pain", "diabetes", "days_hospitalized"]].assign(const=1)
logit_model=sm.Logit(y,X)
result=logit_model.fit()
print(result.summary())

# BACKWARD ELIMINATION # 8: Seek highest p-value above 0.10 (eliminated high_bp, p-value of 0.459)
y = regress_df.arthritis
X = regress_df[["gender_nonbinary", "comp_risk_low", "comp_risk_medium", "back_pain", "diabetes", "days_hospitalized"]].assign(const=1)
logit_model=sm.Logit(y,X)
result=logit_model.fit()
print(result.summary())

# BACKWARD ELIMINATION # 9: Seek highest p-value above 0.10 (eliminated diabetes, p-value of 0.376)
y = regress_df.arthritis
X = regress_df[["gender_nonbinary", "comp_risk_low", "comp_risk_medium", "back_pain", "days_hospitalized"]].assign(const=1)
logit_model=sm.Logit(y,X)
result=logit_model.fit()
print(result.summary())

# BACKWARD ELIMINATION # 10: Seek highest p-value above 0.10 (eliminated comp_risk_low, p-value of 0.203)
y = regress_df.arthritis
X = regress_df[["gender_nonbinary", "comp_risk_medium", "back_pain", "days_hospitalized"]].assign(const=1)
logit_model=sm.Logit(y,X)
result=logit_model.fit()
print(result.summary())

# BACKWARD ELIMINATION # 11: Seek highest p-value above 0.10 (eliminated gender_nonbinary, p-value of 0.207)
y = regress_df.arthritis
X = regress_df[["comp_risk_medium", "back_pain", "days_hospitalized"]].assign(const=1)
logit_model=sm.Logit(y,X)
result=logit_model.fit()
print(result.summary())

# All p-values for independent variables are < 0.10, this is the final reduced model
y = regress_df.arthritis
X = regress_df[["comp_risk_medium", "back_pain", "days_hospitalized"]].assign(const=1)
logit_model=sm.Logit(y,X)
result=logit_model.fit()
print(result.summary())

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
logreg = LogisticRegression()
logreg.fit(X_train, y_train)
y_pred = logreg.predict(X_test)
print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))
final_matrix = confusion_matrix(y_test, y_pred)
print(final_matrix)

result.params

# Calculate odds ratios for each coefficient
print(f"The odds ratio for comp_risk_medium is {round(np.exp(0.073224), 3)}. Given this, the change in odds for arthritis is {round((np.exp(0.073224) - 1) * 100, 3)}")
print(f"The odds ratio for back_pain is {round(np.exp(0.080667), 3)}. Given this, the change in odds for arthritis is {round((np.exp(0.080667) - 1) * 100, 3)}")
print(f"The odds ratio for days_hospitalized is {round(np.exp (0.001541), 3)}. Given this, the change in odds for arthritis is {round((np.exp( 0.001541) - 1) * 100, 3)}")

